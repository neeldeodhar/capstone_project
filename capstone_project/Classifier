import pandas as pd
import numpy as np
from numpy import ndarray
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split 
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
from capstone_project import analyzer
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OrdinalEncoder
#from tensorflow.keras.models import Sequential 

def main():
    absolute_path = "C:/Users/ideod/OneDrive/Documents/new folder zip data/diamonds.csv"
    dfdf = analyzer.read_dataset(csv_file_path=absolute_path)

    data_manipulation = analyzer.DataManipulation(dfdf)
    y = dfdf['cut']    
    selected_features = dfdf[['carat', 'depth', 'table', 'price' ]]
    
    encoder = LabelEncoder()
    Y = encoder.fit_transform(y)
    
    score_dict = {}
    scaler = StandardScaler()
    X = scaler.fit_transform(selected_features)
    X_train, X_test_val, Y_train, Y_test_val = train_test_split(X, Y, test_size=0.2, random_state=45)
    X_test, X_val, Y_test, Y_val = train_test_split(X_test_val, Y_test_val, test_size=0.5, random_state=45)
  
    
    X_trainscale = scaler.fit_transform(X_train)
    X_testscale = scaler.transform(X_test)
    

    LRC = MyLogisticRegression(random_state= 0, params={})
    LRC.fit(X_train, Y_train , None)
    score_dict ["Logistic Regression"] = LRC.score(X_test, Y_test)
    
    print(score_dict)

def score(y_true: np.ndarray, y_predicted: np.ndarray) -> float:
    score = accuracy_score(y_true, y_predicted)
    return score

def plot_confusion_matrix(self):
        CM = confusion_matrix(self.y_test, self.y_pred)
        sns.heatmap(cm, annot = True, cmpap = 'Blues')
        return CM  
class Classifier: 
    def __init__(self, random_state: int, params: dict):
        #self.X_test = X_test
       # self.X_train = X_train
        
        #self.y_test = y_test
       # self.y_train = y_train
       # self.n_estimators = 10
       # self.max_depth = 10
       self.random_state = random_state
       self.model = self.create_model()
       self.params = params
    def create_model():
        return None

    def fit(self, X_train: np.ndarray, y_true_train: np.ndarray, params: dict):
       # self.model = RandomForestClassifier(n_estimators= self.n_estimators, max_depth = self.max_depth, random_state = 42)
        #self.model.fit(self.X_train, self.y_train)
        self.model.fit(X_train, y_true_train)
    def predict(self, X: np.ndarray) -> np.ndarray:
        predict_results = self.model.predict(X)
        return predict_results
    def split_data(self):
        self.X_train = self.X_test, self.Y_train, self.Y_test = train_test_split(X, Y, test_size = self.custom_param, random_state =42)
    def score(self, X:np.ndarray, y_true: ndarray) ->float:
        y_predicted = self.predict(X)
        score = accuracy_score(y_true, y_predicted)
        return score

class MyLogisticRegression(Classifier):
    def __init__(self, random_state: int, params: dict):
        super().__init__(random_state = random_state, params = params)
    def create_model(self)-> LogisticRegression:
        model = LogisticRegression()
        return model

class Random_Classifier(Classifier):
    def __init__(self, n_estimators: int, criterion: str, max_depth: int):
        super().__init__(n_estimators = n_estimators, criterion = criterion, max_depth = max_depth, params = params)
       # self.test_size = test_size
   # def split_data(self):
      #  self.X_train = self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size = self.custom_param, random_state =42)
    def create_model(self)-> RandomForestClassifier:
        RC_model = RandomForestClassifier()
        return RC_model
class DecisionTree(Classifier):
    def __init__(self, criterion:str, random_state:int, max_depth: int):
       # self.criterion = criterion
        #self.random_state = random_state
        #self.max_depth = max_depth
        super().__init__(random_state = random_state, max_depth = max_depth, criterion = criterion, params = params)
    def create_model(self)-> DecisionTreeClassifier:
        DTC = DecisionTreeClassifier (**self.params)
       # DCT.fit(X_train, y_train)
        return DTC   
class SVC_classifier(Classifier):        
    def __init__(self, kernel:str, gamma: str, random_state:int, max_depth: int):
        self.criterion = criterion
        self.random_state = random_state
        self.gamma = gamma
        self.kernel = kernel 
    def SVC (self) -> SVC:
        scoreSVC = []

        svm = SVC(**self.params).fit(X_train, Y_train)
        y_pred = svm.predict(X_test)
        return svm

#class ANN_classifier(Classifier):
   # def __init__(self, activation: str):
       # self.activation = activation

   # def ANN():
       # sq = Sequential()

       # sq.add(Dense(12, input_dim=8, activation='relu'))

       # sq.add(Dense(8, activation='relu'))

      #  sq.add(Dense(1, activation='sigmoid'))

       # sq.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
#class MLP_Classifier(Classifier):
  #  def __init__(self, )

if __name__ =="__main__":
   main()
  #  RFM = Random_Classifier(0.2)
   # RFM.split_data()
    #RFM.fit(RFM.X_train, RFM.y_train)
   # absolute_path = "C:/Users/ideod/OneDrive/Documents/new folder zip data/diamonds.csv"
    #dfdf = read_dataset(csv_file_path=absolute_path)
   # params = {"criterion": "lbgs"}
    #random_state = 4
    #logreg = LogisticRegression (params = params, random_state = random_state)
    #logreg.score
