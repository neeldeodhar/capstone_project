import pandas as pd

import numpy as np

import os

import matplotlib.pyplot as plt

import seaborn as sns

import sys

import scipy.cluster.hierarchy as sch

from sklearn.preprocessing import OrdinalEncoder
from sklearn.preprocessing import OneHotEncoder
from sklearn.cluster import KMeans
from sklearn.cluster import AgglomerativeClustering
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
from capstone_project import analyzer
from sklearn.model_selection import train_test_split 
from sklearn.cluster import MeanShift
from sklearn.datasets import make_blobs


#MAIN FUNCTION: includes reading dataset and preprocessing (encoding, train test split, scaling etc.)

def main():
    absolute_path = "C:/Users/ideod/OneDrive/Documents/new folder zip data/diamonds.csv"
    dfdf = analyzer.read_dataset(csv_file_path=absolute_path)
    data_manipulation = analyzer.DataManipulation(dfdf)

    y = dfdf['cut']    
    selected_features = dfdf[['carat', 'depth', 'table', 'price' ]]
    
    encoder = LabelEncoder()
    Y = encoder.fit_transform(y)
    
    
    scaler = StandardScaler()
    X = scaler.fit_transform(selected_features)
    X_train, X_test_val, Y_train, Y_test_val = train_test_split(X, Y, test_size=0.2, random_state=45)
    X_test, X_val, Y_test, Y_val = train_test_split(X_test_val, Y_test_val, test_size=0.5, random_state=45)
  
    
    X_trainscale = scaler.fit_transform(X_train)
    X_testscale = scaler.transform(X_test)
    
    score_kM = {}
    KM = MyK_Means(params = {} , random_state= None, n_clusters= 2)
   
    KM.fit(X_train, Y_train , None)
    score_kM ["k Means"] = KM.predict(X_test)
    
    print(score_kM)
    
 

    score_MS = {}
    MS = My_MeanShift(params = {} , random_state= None, n_clusters= 2)
    MS.fit(X_train, Y_train , None)
    score_MS ["Mean shift Clustering"] = MS.predict(X_test)
    
    print(score_MS)
    
# HIERARCHICAL CLUSTERING (AGGLOMERATIVE) MODEL HAS BEEN CREATED BELOW; HOWEVER NOT CALLED ABOVE DUE TO MEMORY ISSUES ON COMPUTER    

class Clusterer():
    def __init__(self, random_state: int, params: dict):
      
       self.random_state = random_state
      
       self.params = params
       self.model = self.create_model()

    def fit(self, X_train: np.ndarray, y_true_train: np.ndarray, params: dict):
       
        self.model.fit(X_train, y_true_train)
    def predict(self, X: np.ndarray) -> np.ndarray:
        predict_results = self.model.predict(X)
        return predict_results

class MyK_Means(Clusterer):
    def __init__(self, params: dict, random_state: int, n_clusters: int):
        super().__init__(random_state = random_state, params = params)
    def create_model(self)-> KMeans:
        score = []
        X, _ = make_blobs(n_samples= 300, centers = 4, cluster_std = 0.60, random_state = 0)
        for i in range(1, 10):
            model = KMeans(n_clusters=i, random_state=0).fit(X)
            score.append(model.inertia_)
            print(score)
     
#PLOTTING TO FIND THE OPTIMAL VALUE USING ELBOW METHOD BELOW     
          
        plt.plot(range(1, 10), score, marker='.', markersize=10)
        plt.title('The Elbow Method')
        plt.xlabel('Number of clusters')
        plt.ylabel('SSE') # Model Inertia
        plt.show()
        return model
#creating Agglomerative model
class MyAgglomerative(Clusterer):
    def __init__(self, random_state: int, n_clusters: int, params: dict):
        super().__init__(random_state = random_state, params = params)
    def create_model(self)-> AgglomerativeClustering:
        model = AgglomerativeClustering()
        return model
#creating Meanshift model
class My_MeanShift(Clusterer):
    def __init__(self, random_state: int, n_clusters: int, params: dict):
        super().__init__(random_state = random_state, params = params)
    def create_model(self)-> KMeans:
        model = KMeans()
        return model

if __name__ =="__main__":
   main()